<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://ym-xu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ym-xu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-11-08T07:58:52+00:00</updated><id>https://ym-xu.github.io/feed.xml</id><title type="html">blank</title><subtitle>Msc CS</subtitle><entry><title type="html">Linguistic Unit Discovery</title><link href="https://ym-xu.github.io/blog/2022/Linguistic-Unit-Discovery/" rel="alternate" type="text/html" title="Linguistic Unit Discovery"/><published>2022-06-30T16:40:16+00:00</published><updated>2022-06-30T16:40:16+00:00</updated><id>https://ym-xu.github.io/blog/2022/Linguistic%20Unit%20Discovery</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/Linguistic-Unit-Discovery/"><![CDATA[<h1 id="overview">Overview</h1> <p>This paper summarizes the accomplishments of “Speaking Rosetta” workshop, focusing on replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.</p> <h1 id="motivation">motivation</h1> <p>To develop speech and language technology (SLT) large amounts of annotated data are required. However, many languages do not have enough speech data to train ASR system. Moreover, an estimated half of the human languages do not have orthography, and many others do not use it in a consistent fashion.</p> <p>Considering the fact when children learn language, they not only use speech but also learn it form raw sensory signals (e.g. visual information), a new strand of research has emerged. It uses visual information, from images, to discover word-like units from the speech signal using speech-image associations.</p> <p>The workshop explored the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography, through replacing the orthographic transcriptions typically used for training an ASR system by images and/or translations in a well-resourced language. This paper focus on 4 tasks: two with symbolic units (unit discovery and speech synthesis) and two end-to-end tasks without the need for explicit symbolic units (speech2image and speech2translation).</p> <h1 id="speaking-rosetta">SPEAKING ROSETTA</h1> <p><img src="https://ym-xu.github.io/assets/ref/Speaking%20Rosetta%20project.jpg" alt="alt img"/></p> <p>Fig 1 is the visual representation of the end-to-end systems, and structure, of the Rosetta project.</p> <h2 id="dataset">Dataset</h2> <ul> <li> <p>Mboshi</p> </li> <li>FlickR-real speech</li> <li>SPEECH-COCO-synthetic</li> <li>How-To dataset</li> <li>Spoken Dutch Corpus</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>BLEU</li> <li>error rates</li> <li>word discovery metrics</li> </ul> <h2 id="xnmt-toolkit">XNMT Toolkit</h2> <p>Useful, XNMT is a sequence-to-sequence neural network toolkit which reads in a sequence of (variable-length) inputs, and then generates a different sequence of (variable-length) output.</p> <h2 id="now-i-just-focus-on-image-and-linguistics">Now I just focus on Image and linguistics</h2> <h3 id="speech-to-image">Speech-to-Image</h3> <p>A S2I system learns to map images and speech to the same embedding space, ad retrieves an image using spoken captions. And while doing so, it uses multimodel input to discover speech units in an unsupervised manner.</p> <h3 id="image-to-speech">Image-to-Speech</h3> <p>Similiar to Image Caption. XNMT accepts image feature vectors as inputs, and generates speech units as output, which were sent to TTS. Four types of intermediate speed unit were tested:</p> <ul> <li>L1-words</li> <li>L1-phones, generated using a same-language ASR, which provides an upper bound performance</li> <li>L2-phones from the cross-language definition of units approach</li> <li>pseudo-phones generated using AUD</li> </ul>]]></content><author><name></name></author><category term="Literature-Notes"/><category term="Multimodal-Learning"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</title><link href="https://ym-xu.github.io/blog/2022/data2vec/" rel="alternate" type="text/html" title="data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"/><published>2022-06-21T16:40:16+00:00</published><updated>2022-06-21T16:40:16+00:00</updated><id>https://ym-xu.github.io/blog/2022/data2vec</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/data2vec/"><![CDATA[<h2 id="overview">Overview</h2> <p>Data2vec is a framework that uses the same learning method for either speech, NLP or computer vision.</p> <p>It uses a standard Transformer architecture, predicts latent representations of the full input data based on a masked view of the input in a selfdistillation setup of Transformer.</p> <p>The aim for this framework is to predict contextualized latent representations that contain information from the entire input.</p> <h2 id="method">Method</h2> <p><img src="https://ym-xu.github.io/assets/ref/data2vec1.jpg" alt="alt img"/></p> <p>data2vec is trained by predicting the model representations of the full input data given a partial view of the input.</p> <p>First, encode a masked version of training sample (model in student mode).</p> <p>Then, construct training targets by encoding the unmasked version of the input sample with the same model but when parameterized as an exponentially moving average of the model weights (model in teacher mode).</p> <p>The <strong>target representations encode</strong> all of the information in the training sample and the learning task is <strong>for the student to predict these representations given a partial view of the input</strong>.</p> <h3 id="teacher-parameterization">Teacher parameterization</h3> <p>The encoding of the unmasked training sample is parameterized by an exponentially moving average (EMA) of the model parameters \(\theta\), the weight of the model in target-mode \(\Delta\) is:</p> \[\Delta \gets \tau \Delta + (1-\tau)\theta\] <p>\(\tau\) is a schedule, this parameter linearly increases from \(\tau _{0}\) to \(\tau _{e}\) over the first \(\tau _n\) updates, after that the value is kept constant for the remainder of training.</p>]]></content><author><name></name></author><category term="Literature-Notes"/><category term="Multimodal-Learning"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval.</title><link href="https://ym-xu.github.io/blog/2022/GCN-T&I-CMIR/" rel="alternate" type="text/html" title="Modeling Text with Graph Convolutional Network for Cross-Modal Information Retrieval."/><published>2022-06-13T16:40:16+00:00</published><updated>2022-06-13T16:40:16+00:00</updated><id>https://ym-xu.github.io/blog/2022/GCN%20T&amp;I%20CMIR</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/GCN-T&amp;I-CMIR/"><![CDATA[<h2 id="overview">Overview</h2> <p><strong>What is Cross-modal Information Retrieval?</strong></p> <ul> <li>CMIR enables users to take a query of one modality to retrieve data in relevant content in other modalities.</li> </ul> <p>This paper model texts by graphs using similarity measure based on word2vec. The authors build a dual-path neural network to learn the couple features in cross-modal information retrival.</p> <p>In text modeling, they use GCN, and in image modeling, they use neural network with layers of nonlinearities based on off-the-shelf features.</p> <h2 id="motivation-and-contributions">Motivation and Contributions</h2> <p><strong>To solve the Challenges:</strong></p> <ul> <li>Heterogeneous gap and semantic gap between different modality features, no natural correspondence between different modalities.</li> <li>Although existed text feature extraction methods like word2vec is enriched by learning from neighboring words, it still ignores the global structural information.</li> </ul> <p><strong>Contributions</strong></p> <ul> <li>Model text data by GCN, which realizes the cross-modal retrieval between irregular graph-structured data and regular grid-structured data;</li> <li>Jointly learn the textual and visual representations as well as text-image similarity metric, providing an end-to-end training model;</li> <li>Experimental on Eng-Wiki, NUS-WIDE, Pascal, TVGraz and Ch-Wiki.</li> </ul> <h2 id="methodology">Methodology</h2> <h3 id="text-modeling">Text Modeling</h3> <p>Words: \(W = [w_1,w_2,...,w_N]\)</p> <p>Graph is a k-nearest neighbor graph - \(G = (V,E)\) . vertex \(v_i \in V\) is corresponding to a unique word and each edge \(e_{ij} \in E\) is defined by the word2vec similarity between two words.</p> \[e_{ij} = \begin{cases} 1, &amp; \text{if $w_i \in N_{k}(w_{j}) $ or $w_j \in N_{k}(w_{i}) $ } \\ 0, &amp; \text{otherwise} \end{cases}\] <p>k = 8, graph structure is \(A \in \mathbb{R}^{N \times N}\) , graph features is \(bag-of-words\) vector and the frequency value of word \(w_i\) serves as the 1-dimensional feature on vertex \(v_i\).</p> <h2 id="graph-convolutional">Graph Convolutional</h2> <p>Given a text’s input graph feature vector \(F_{in}\) , output \(F_{out}\).</p> <p><strong>First</strong>, \(F_{in}\) is transformed to the spectral domain via graph Fourier transform, based on the normalized graph Laplacian:</p> \[L = I_N - D^{-1/2}AD^{-1/2}\] <p>\(I_N\) and $D$ are respectively the identity matrix and diagonal degree matrix of the graph structure $G$.</p> <p><strong>Then</strong> , \(L\) can be eigendecomposed as \(L = U \Lambda U^{T}\), \(U\) is a set of eigenvectors and \(\Lambda\) is a set of real, non-negative eigenvalues. The Fourier transform of \(F_{in}\) is a function \(U\):</p> \[\widehat{F_{in}} = U^T F_{in}\] <p>Inverse Transform:</p> \[F_{in} = U \widehat{F_{in}}\] <p>Convolution of \(F_{in}\) with a spectral filter \(g_{\theta}\) is:</p> \[F_{out} = g_{\theta} * F_{in} = U g_{\theta}U^{T}F_{in}\] <p>\(\theta\) is a vector to learn:</p> \[g_{\theta} = \sum_{k = 0}^{K-1} \theta_k T_k (\widetilde{L})\] <p>where \(T_{k}(x) = 2xT_{k-1}(x) - T_{k-2}(x)\) , \(T_{0}(x) = 1\) , \(T_{1}(x) = x\), \(\widetilde{L} = 2/\lambda_{max}L - I_N\) , \(\lambda_{max}\) is the lengest enginvalue of \(L\) .</p> <p><strong>Then</strong>, \(F_{out} = g_{\theta}F_{in}\) .</p>]]></content><author><name></name></author><category term="Literature-Notes"/><category term="Crossmodal-Retrieval"/><category term="GCN"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Cross-modal Graph Matching Network for Image-text Retrieval</title><link href="https://ym-xu.github.io/blog/2022/CGMN/" rel="alternate" type="text/html" title="Cross-modal Graph Matching Network for Image-text Retrieval"/><published>2022-05-21T16:40:16+00:00</published><updated>2022-05-21T16:40:16+00:00</updated><id>https://ym-xu.github.io/blog/2022/CGMN</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/CGMN/"><![CDATA[<h3 id="overview">Overview</h3> <p>Existing image-text retrieval methods:</p> <ul> <li> <p>independent representation matching methods, which generate the embeddings of images and sentences independently and thus are convenient for retrieval with hand-crafted matching measures.</p> </li> <li> <p>cross-interaction matching methods, which achieve improvement by introducing the interaction-based networks for inter-relation reasoning, yet suffer the low retrieval efficiency.</p> </li> </ul> <p>This paper proposes a graphbased Cross-modal Graph Matching Network (CGMN), which explores both intra- and inter-relations without introducing network interaction.</p> <h3 id="motivation-and-contributions">Motivation and Contributions</h3> <p><img src="https://ym-xu.github.io/assets/ref/CGMN1.jpg" alt="alt img"/></p> <p>As shown in the figure, image-text retrieval methods can be classified into two categories, including independent representation matching methods and cross-interaction matching methods.</p> <p>However, independent representation matching methods sacrificing some accuracy because the matching step only needs to compute the embedding distances between the query and each pre-stored image or sentence embedding in the database. And cross-interaction matching methods have low computational efficiency because they need more similarity computing in an interactive manner or network-based matching.</p> <p>Therefore, this paper develop an effective and efficient image-text matching method, which can achieve good accuracy as cross-combined matching methods, while being as efficient as independent representation matching methods.</p> <p><strong>Contributions</strong></p> <ul> <li> <p>The authors propose a novel graph-based independent representation method CGMN for fine-grained and fast image-text retrieval, which is computationally eficient as independent representation methods while taking the advantage of cross-modal inter-relation reasoning of cross-interaction methods.</p> </li> <li> <p>They design a graph-based network to achieve intra-relation reasoning in embedding images and sentences and propose a novel graph node matching loss only used during training, to better learn cross-modal fine-grained alignment and achieve inter-relation reasoning between image regions and words in sentences, without any sacrifice of computational efficiency in the retrieval.</p> </li> </ul>]]></content><author><name></name></author><category term="Literature-Notes"/><category term="Crossmodal-Retrieval"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval</title><link href="https://ym-xu.github.io/blog/2022/SGM-CMIR/" rel="alternate" type="text/html" title="Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval"/><published>2022-05-18T16:40:16+00:00</published><updated>2022-05-18T16:40:16+00:00</updated><id>https://ym-xu.github.io/blog/2022/SGM-CMIR</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/SGM-CMIR/"><![CDATA[<h2 id="overview">Overview</h2> <p>Natural scenes image mainly involves two kinds of visual concepts: object and their relations, which are equally essential to image-text retrieval. Intuitively, compared to CNN-based methods, Graph-based image feature contains more important visual information and considers the relationship between objects.</p> <p>This paper use represent image and text with respectively visual scene graph (VSG) and textual scene graph (TSG), then CMIR tasks is naturally formulated as cross-modal scene graph matching.</p> <p>This method enables us to obtain both object-level and relationship-level cross-modal features, which favorably enables us to evaluate the similarity of image and text in the two levels in a more plausible way.</p> <h2 id="motivation-and-contributions">Motivation and Contributions</h2> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR0.jpg" alt="alt img"/></p> <p>As shown in the top of the figure, early approaches use global representations to express the whole image and sentence, which ignore the local details. These methods work well on simple cross-modal retrieval scenario, but not performance good at more realistic cases that involve complex natural scenes.</p> <p>Recent studies pay attention to local detailed matching by detecting objects in both images and text, but ignore the relationships.</p> <p>In this paper, authors organize the objects and the relationships into scene graphs for both modalities, converting the conventional image-text retrieval problem to the matching of two scene graphs.</p> <p><strong>Contributions</strong></p> <ul> <li>Extract objects and relationships from the image and text to form the VSG and TSG, and design a so-called Scene Graph Matching (SGM) model.</li> <li>The VSG encoder is a Multi-modal Graph Convolution Network (MGCN), which enhances the representations of each node on the VSG by aggregating useful information from other nodes and updates the object and relationship features in different manners.</li> <li>The TSG encoder contains two different bi-GRUs aiming to encode the object and relationship features, respectively.</li> <li>Both object-level and relationship-level features are learned in each graph, and the two feature graphs corresponding to two modalities can be finally matched at two levels in a more plausible way.</li> </ul> <h2 id="methodology">Methodology</h2> <h3 id="visual-scene-graph-generation">Visual Scene Graph Generation</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR2.jpg" alt="alt img"/></p> <p>Given a raw image, represent a visual scene graph as \(G = \{V, E\}\), \(V\) is the node-set, and $E$ is the edge-set. As shown in figure, the pink rectangles denote object nodes, each of which corresponds to a region of the image. The ellipses in light blue are relationship nodes, each of which connects two object nodes by directed edges.</p> <p>If there are \(N_o\) object nodes and \(N_r\) relationship nodes in VSG:</p> <p>Object nodes set:</p> \[O = \{o_{i}|i = 1,2,...,N_o \}\] <p>Relationship nodes set: \(R = \{r_{ij}\} \subseteq O \times O\), \(\vert R \vert = N_r\), \(r_{ij}\) is the relationship of \(o_i\) and \(o_j\)</p> <h3 id="visual-scene-graph-encoder">Visual Scene Graph Encoder</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR1.jpg" alt="alt img"/></p> <p><strong>Visual Feature Extractor.</strong> The pre-trained visual feature extractor: emcoding image regions into feature vector (Faster-RCNN). Each node in VSG will be encoded into a \(d_1\)-dimension visual feature.</p> <p>For object node \(o_i\), visual feature vector \(v_{oi}\) is extracted from its corresponding image region.</p> <p>For relationship node \(r_{ij}\) , visual feature vector \(r_{ij}\) is extracted from the union image region of \(o_i\) and \(o_j\) .</p> <p><strong>Label Embedding Layer.</strong></p> <p>Given one-hot vectors \(l_{oi}\) and \(l_{r_{ij}}\), output \(e_{oi}\) and \(e_{r_{ij}}\) :</p> <p>\(W_o \in R^{d_2 \times C_o}\) and \(W_r \in R^{d_2 \times C_r}\) are trainable parameters and initialized by word2vec (\(d_2=300\)). \(C_o\) is the category number of objects and $C_r$ is the category number of relationships.</p> \[e_{oi} = W_oI_{oi} , W_o \in R^{d_2 \times C_o}\] \[e_{r_{ij}} = WrI_{r_{ij}}, W_r \in R^{d_2 \times C_r}\] <p><strong>Multi-modal Fusion Layer.</strong></p> <p>\(W_u \in R^{d_1 \times (d_1 + d_2)}\) is the trainable parameter of fusion layer.</p> <p>\(u_{oi} = tanh(W_u[v_{oi},e_{oi}])\),</p> \[u_{r_{ij}} = tanh(W_u[v_{r_{ij}},e_{r_{ij}}])\] <h3 id="visual-scene-graph-encoder-1">Visual Scene Graph Encoder</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR3.jpg" alt="alt img"/></p> <p>Two relationshps: word order (black arrows) and semantic relationship (brown arrows, are built from SPICE)</p> <p>The textual scene graph encoder consists of a word embedding layer, a word-level bi-GRU encoder, and a path-level biGRU encoder.</p> <p>Build TSG:</p> <p>Fiest, each word $w_i$ in the sentences is embedded into a vector by the word embedding layer as \(e_{w_{i}} = W_el_{w_{i}}\) . (\(l_{w_{i}}\) is the one-hot vector of \(w_i\) , \(W_e\) is the parameter matrix of embedding layer, initialisation as the same word2vec in VSG encoder and be learned during training end-to-end).</p> <p>Second, two kinds of path are encoded separately by different bi-GRUs.</p> <p>For the word-order path:</p> \[\overrightarrow{h_{w_{i}}} = \overrightarrow{GRU_w}(e_{w_{i}},\overrightarrow{h_{w_{i-1}}})\] \[\overleftarrow{h_{w_{i}}} = \overleftarrow{GRU_w}(e_{w_{i}},\overleftarrow{h_{w_{i+1}}})\] \[h_{w_{i}} = (\overrightarrow{h_{w_{i}}} + \overleftarrow{h_{w_{i}}})/2\] <p>For the $N_p$ semantic relationship paths:</p> <p>\(h_{p{i}} = (\overrightarrow{GRU_p}(path_i) + \overleftarrow{GRU_p}(path_i))/2, i \in [1,N_p]\) , the last hidden state feature of \(i-th\) semantic relationship path, which is also a relationship feature of the TSG.</p> <h3 id="similarity-function">Similarity Function</h3> <p>Each graph has two levels of features, this work match them respectively.</p> <p>Setting:</p> <ul> <li>there are \(N_o\) and \(N_w\) object features in the visual and textual feature graph, each of them is a $D$-dimension vector.</li> <li>for feature vectors $h_i$ and $h_j$ , the similarity score is \(h_{i}^{T}h_{j}\), the similarity scores of V and T objects, it’s a \(N_w \times N_o\) matrix.</li> <li>find the maximum value of each row, which means for very textual object, the most related visual objects among $N_o$ visual objects is picked up.</li> <li>similarity score: <ul> <li> \[S^{o} = (\sum_{t=1}^{N_w} \max_{i \in [1, N_o]} h_{w_t}^{T}h_{o_i})/N_{w}\] </li> <li> \[S^{r} = (\sum_{t=1}^{N_p} \max_{r_{ij} \in R} h_{p_t}^{T}h_{r_{ij}})/N_{p}\] </li> <li> \[S = S^o + S^r\] </li> </ul> </li> </ul> <h3 id="loss-function">Loss Function</h3> <p>usually, use triplet loss:</p> \[L(k,l) = \sum_{\hat{l}} max(0,m- S_{k \hat{l}} + S_{k\hat{l}}) + \sum_{\hat{k}} max(0,m- S_{k \hat{l}} + S_{k\hat{l}})\] <p>Consider to the influence of hardest negative samples,</p> \[L_{+}(k,l) = max(0, m - S_{kl} + S_{k \hat{l}}) + max(0, m - S_{kl} + S_{\hat{k} l})\] <p>where \(\hat{l} = argmax_{j \not = l}S_{kj}$ and $k' = argmax_{j \not = k}S_{jl}\) are hardest negative in the mini-batch.</p>]]></content><author><name></name></author><category term="Literature-Notes"/><category term="Crossmodal-Retrieval"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://ym-xu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://ym-xu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://ym-xu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>
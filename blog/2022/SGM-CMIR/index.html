<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval | Yiming Xu</title> <meta name="author" content="Yiming Xu"/> <meta name="description" content="Msc CS"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ˜º</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ym-xu.github.io/blog/2022/SGM-CMIR/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">YimingÂ </span>Xu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval</h1> <p class="post-meta">May 18, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> Â  Â· Â  <a href="/blog/tag/Crossmodal-Retrieval"> <i class="fas fa-hashtag fa-sm"></i> Crossmodal-Retrieval</a> Â  Â  Â· Â  <a href="/blog/category/Literature-Notes"> <i class="fas fa-tag fa-sm"></i> Literature-Notes</a> Â  </p> </header> <article class="post-content"> <h2 id="overview">Overview</h2> <p>Natural scenes image mainly involves two kinds of visual concepts: object and their relations, which are equally essential to image-text retrieval. Intuitively, compared to CNN-based methods, Graph-based image feature contains more important visual information and considers the relationship between objects.</p> <p>This paper use represent image and text with respectively visual scene graph (VSG) and textual scene graph (TSG), then CMIR tasks is naturally formulated as cross-modal scene graph matching.</p> <p>This method enables us to obtain both object-level and relationship-level cross-modal features, which favorably enables us to evaluate the similarity of image and text in the two levels in a more plausible way.</p> <h2 id="motivation-and-contributions">Motivation and Contributions</h2> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR0.jpg" alt="alt img"></p> <p>As shown in the top of the figure, early approaches use global representations to express the whole image and sentence, which ignore the local details. These methods work well on simple cross-modal retrieval scenario, but not performance good at more realistic cases that involve complex natural scenes.</p> <p>Recent studies pay attention to local detailed matching by detecting objects in both images and text, but ignore the relationships.</p> <p>In this paper, authors organize the objects and the relationships into scene graphs for both modalities, converting the conventional image-text retrieval problem to the matching of two scene graphs.</p> <p><strong>Contributions</strong></p> <ul> <li>Extract objects and relationships from the image and text to form the VSG and TSG, and design a so-called Scene Graph Matching (SGM) model.</li> <li>The VSG encoder is a Multi-modal Graph Convolution Network (MGCN), which enhances the representations of each node on the VSG by aggregating useful information from other nodes and updates the object and relationship features in different manners.</li> <li>The TSG encoder contains two different bi-GRUs aiming to encode the object and relationship features, respectively.</li> <li>Both object-level and relationship-level features are learned in each graph, and the two feature graphs corresponding to two modalities can be finally matched at two levels in a more plausible way.</li> </ul> <h2 id="methodology">Methodology</h2> <h3 id="visual-scene-graph-generation">Visual Scene Graph Generation</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR2.jpg" alt="alt img"></p> <p>Given a raw image, represent a visual scene graph as \(G = \{V, E\}\), \(V\) is the node-set, and $E$ is the edge-set. As shown in figure, the pink rectangles denote object nodes, each of which corresponds to a region of the image. The ellipses in light blue are relationship nodes, each of which connects two object nodes by directed edges.</p> <p>If there are \(N_o\) object nodes and \(N_r\) relationship nodes in VSG:</p> <p>Object nodes set:</p> \[O = \{o_{i}|i = 1,2,...,N_o \}\] <p>Relationship nodes set: \(R = \{r_{ij}\} \subseteq O \times O\), \(\vert R \vert = N_r\), \(r_{ij}\) is the relationship of \(o_i\) and \(o_j\)</p> <h3 id="visual-scene-graph-encoder">Visual Scene Graph Encoder</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR1.jpg" alt="alt img"></p> <p><strong>Visual Feature Extractor.</strong> The pre-trained visual feature extractor: emcoding image regions into feature vector (Faster-RCNN). Each node in VSG will be encoded into a \(d_1\)-dimension visual feature.</p> <p>For object node \(o_i\), visual feature vector \(v_{oi}\) is extracted from its corresponding image region.</p> <p>For relationship node \(r_{ij}\) , visual feature vector \(r_{ij}\) is extracted from the union image region of \(o_i\) and \(o_j\) .</p> <p><strong>Label Embedding Layer.</strong></p> <p>Given one-hot vectors \(l_{oi}\) and \(l_{r_{ij}}\), output \(e_{oi}\) and \(e_{r_{ij}}\) :</p> <p>\(W_o \in R^{d_2 \times C_o}\) and \(W_r \in R^{d_2 \times C_r}\) are trainable parameters and initialized by word2vec (\(d_2=300\)). \(C_o\) is the category number of objects and $C_r$ is the category number of relationships.</p> \[e_{oi} = W_oI_{oi} , W_o \in R^{d_2 \times C_o}\] \[e_{r_{ij}} = WrI_{r_{ij}}, W_r \in R^{d_2 \times C_r}\] <p><strong>Multi-modal Fusion Layer.</strong></p> <p>\(W_u \in R^{d_1 \times (d_1 + d_2)}\) is the trainable parameter of fusion layer.</p> <p>\(u_{oi} = tanh(W_u[v_{oi},e_{oi}])\),</p> \[u_{r_{ij}} = tanh(W_u[v_{r_{ij}},e_{r_{ij}}])\] <h3 id="visual-scene-graph-encoder-1">Visual Scene Graph Encoder</h3> <p><img src="https://ym-xu.github.io/assets/ref/SGM-CMIR3.jpg" alt="alt img"></p> <p>Two relationshps: word order (black arrows) and semantic relationship (brown arrows, are built from SPICE)</p> <p>The textual scene graph encoder consists of a word embedding layer, a word-level bi-GRU encoder, and a path-level biGRU encoder.</p> <p>Build TSG:</p> <p>Fiest, each word $w_i$ in the sentences is embedded into a vector by the word embedding layer as \(e_{w_{i}} = W_el_{w_{i}}\) . (\(l_{w_{i}}\) is the one-hot vector of \(w_i\) , \(W_e\) is the parameter matrix of embedding layer, initialisation as the same word2vec in VSG encoder and be learned during training end-to-end).</p> <p>Second, two kinds of path are encoded separately by different bi-GRUs.</p> <p>For the word-order path:</p> \[\overrightarrow{h_{w_{i}}} = \overrightarrow{GRU_w}(e_{w_{i}},\overrightarrow{h_{w_{i-1}}})\] \[\overleftarrow{h_{w_{i}}} = \overleftarrow{GRU_w}(e_{w_{i}},\overleftarrow{h_{w_{i+1}}})\] \[h_{w_{i}} = (\overrightarrow{h_{w_{i}}} + \overleftarrow{h_{w_{i}}})/2\] <p>For the $N_p$ semantic relationship paths:</p> <p>\(h_{p{i}} = (\overrightarrow{GRU_p}(path_i) + \overleftarrow{GRU_p}(path_i))/2, i \in [1,N_p]\) , the last hidden state feature of \(i-th\) semantic relationship path, which is also a relationship feature of the TSG.</p> <h3 id="similarity-function">Similarity Function</h3> <p>Each graph has two levels of features, this work match them respectively.</p> <p>Setting:</p> <ul> <li>there are \(N_o\) and \(N_w\) object features in the visual and textual feature graph, each of them is a $D$-dimension vector.</li> <li>for feature vectors $h_i$ and $h_j$ , the similarity score is \(h_{i}^{T}h_{j}\), the similarity scores of V and T objects, itâ€™s a \(N_w \times N_o\) matrix.</li> <li>find the maximum value of each row, which means for very textual object, the most related visual objects among $N_o$ visual objects is picked up.</li> <li>similarity score: <ul> <li> \[S^{o} = (\sum_{t=1}^{N_w} \max_{i \in [1, N_o]} h_{w_t}^{T}h_{o_i})/N_{w}\] </li> <li> \[S^{r} = (\sum_{t=1}^{N_p} \max_{r_{ij} \in R} h_{p_t}^{T}h_{r_{ij}})/N_{p}\] </li> <li> \[S = S^o + S^r\] </li> </ul> </li> </ul> <h3 id="loss-function">Loss Function</h3> <p>usually, use triplet loss:</p> \[L(k,l) = \sum_{\hat{l}} max(0,m- S_{k \hat{l}} + S_{k\hat{l}}) + \sum_{\hat{k}} max(0,m- S_{k \hat{l}} + S_{k\hat{l}})\] <p>Consider to the influence of hardest negative samples,</p> \[L_{+}(k,l) = max(0, m - S_{kl} + S_{k \hat{l}}) + max(0, m - S_{kl} + S_{\hat{k} l})\] <p>where \(\hat{l} = argmax_{j \not = l}S_{kj}$ and $k' = argmax_{j \not = k}S_{jl}\) are hardest negative in the mini-batch.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2022 Yiming Xu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>